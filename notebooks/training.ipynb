{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a44d7202-0ab7-4f39-9302-9e796776b708",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f2250cb-4f8a-4505-a7ad-3e364ac75d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e6f069-a708-411d-85bb-2b83e3efad09",
   "metadata": {},
   "source": [
    "### Importing Classic ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63573844-8c84-4364-ac0e-7df2c742efcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b5a08d-f4a7-4425-9d79-01abb2c4ab37",
   "metadata": {},
   "source": [
    "## 2. Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6cb9aa3-c3f2-48f4-adff-403c4757f400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data loaded successfully.\n",
      "        age  trestbps      chol   thalach   oldpeak  sex_1  cp_1  cp_2  cp_3  \\\n",
      "0 -0.267966 -0.376556 -0.667728  0.806035 -0.037124    1.0   0.0   0.0   0.0   \n",
      "1 -0.157260  0.478910 -0.841918  0.237495  1.773958    1.0   0.0   0.0   0.0   \n",
      "2  1.724733  0.764066 -1.403197 -1.074521  1.342748    1.0   0.0   0.0   0.0   \n",
      "3  0.728383  0.935159 -0.841918  0.499898 -0.899544    1.0   0.0   0.0   0.0   \n",
      "4  0.839089  0.364848  0.919336 -1.905464  0.739054    0.0   0.0   0.0   0.0   \n",
      "\n",
      "   fbs_1  ...  slope_1  slope_2  ca_1  ca_2  ca_3  ca_4  thal_1  thal_2  \\\n",
      "0    0.0  ...      0.0      1.0   0.0   1.0   0.0   0.0     0.0     0.0   \n",
      "1    1.0  ...      0.0      0.0   0.0   0.0   0.0   0.0     0.0     0.0   \n",
      "2    0.0  ...      0.0      0.0   0.0   0.0   0.0   0.0     0.0     0.0   \n",
      "3    0.0  ...      0.0      1.0   1.0   0.0   0.0   0.0     0.0     0.0   \n",
      "4    1.0  ...      1.0      0.0   0.0   0.0   1.0   0.0     0.0     1.0   \n",
      "\n",
      "   thal_3  target  \n",
      "0     1.0       0  \n",
      "1     1.0       0  \n",
      "2     1.0       0  \n",
      "3     1.0       0  \n",
      "4     0.0       0  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df = pd.read_csv('../data/processed/cleaned_data.csv')\n",
    "    print(\"Processed data loaded successfully.\")\n",
    "    print(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'cleaned_data.csv' not found in 'data/processed/'.\")\n",
    "    print(\"Please ensure you have run the preprocessing script first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95ba3b6-2d31-4fa2-af46-978d52737c3a",
   "metadata": {},
   "source": [
    "## 3. Seperating Features and Target & Spliting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26874c10-7b53-4547-8c6d-92b8fa3adb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (241, 22)\n",
      "Testing set shape: (61, 22)\n"
     ]
    }
   ],
   "source": [
    "X= df.drop('target', axis=1)\n",
    "y= df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720de0a6-99b0-4ab1-9af5-c92706069f13",
   "metadata": {},
   "source": [
    "## 4. Model Development & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bd59804-95f5-4327-bf6e-f94566ede88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression\n",
      "Accuracy: 0.85\n",
      "Precision: 0.88\n",
      "Recall: 0.85\n",
      "F1-Score: 0.86\n",
      "\n",
      "\n",
      "Model: K-Nearest Neighbors\n",
      "Accuracy: 0.80\n",
      "Precision: 0.86\n",
      "Recall: 0.76\n",
      "F1-Score: 0.81\n",
      "\n",
      "\n",
      "Model: Support Vector Machine\n",
      "Accuracy: 0.80\n",
      "Precision: 0.86\n",
      "Recall: 0.76\n",
      "F1-Score: 0.81\n",
      "\n",
      "\n",
      "Model: Naive Bayes\n",
      "Accuracy: 0.85\n",
      "Precision: 0.85\n",
      "Recall: 0.88\n",
      "F1-Score: 0.87\n",
      "\n",
      "\n",
      "Model: Decision Tree\n",
      "Accuracy: 0.66\n",
      "Precision: 0.68\n",
      "Recall: 0.70\n",
      "F1-Score: 0.69\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\praja\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] The system cannot find the file specified\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"C:\\Users\\praja\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\praja\\anaconda3\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\praja\\anaconda3\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"C:\\Users\\praja\\anaconda3\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Random Forest\n",
      "Accuracy: 0.82\n",
      "Precision: 0.84\n",
      "Recall: 0.82\n",
      "F1-Score: 0.83\n",
      "\n",
      "\n",
      "Model: Grandient Boosting\n",
      "Accuracy: 0.77\n",
      "Precision: 0.81\n",
      "Recall: 0.76\n",
      "F1-Score: 0.78\n",
      "\n",
      "\n",
      "Model: XGBoost\n",
      "Accuracy: 0.77\n",
      "Precision: 0.83\n",
      "Recall: 0.73\n",
      "F1-Score: 0.77\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models ={\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Support Vector Machine\": SVC(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Grandient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred= model.predict(X_test)\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "    print(f\"Precision: {precision_score(y_test, y_pred):.2f}\")\n",
    "    print(f\"Recall: {recall_score(y_test, y_pred):.2f}\")\n",
    "    print(f\"F1-Score: {f1_score(y_test, y_pred):.2f}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4eb70e0-7bf5-4287-9457-c66f96403420",
   "metadata": {},
   "source": [
    "## 5. Hyper Parameter Tuning For Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6d710b7-9ac9-4f6a-bd21-59b43a65c0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Starting Hyperparameter Tuning --\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best Parameters for Logistic Regression: {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Best Cross-Validation Accuracy: 0.8299\n"
     ]
    }
   ],
   "source": [
    "print(\"-- Starting Hyperparameter Tuning --\")\n",
    "\n",
    "log_reg= LogisticRegression(max_iter=2000, random_state=42)\n",
    "\n",
    "# Defining the hyperparamter grid to search\n",
    "# C--> Inverse of regularisation strength. Smaller values means stronger regularisation\n",
    "# penalty--> Specifies the norm used in the penalisation.\n",
    "# solver--> Algorithm to use in the optimisation problem.\n",
    "param_grid_lr= {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "# Setting up GridSeachCV\n",
    "grid_search_lr= GridSearchCV(estimator=log_reg, param_grid=param_grid_lr, cv=5, scoring='accuracy', n_jobs= -1, verbose=1)\n",
    "\n",
    "# Fitting the grid search to the data\n",
    "grid_search_lr.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best Parameters for Logistic Regression: {grid_search_lr.best_params_}\")\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search_lr.best_score_:.4f}\")\n",
    "\n",
    "# Save best model found by the search\n",
    "best_lr_model = grid_search_lr.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2284d8a8-49f2-43b6-afa9-8402c3ea0471",
   "metadata": {},
   "source": [
    "## 6. Hyper Paramater Tuning For Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "260e67c5-9873-4992-9e1d-c5632a838216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Tuning Naive Bayes --\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Best Parameters for Naive Bayes: {'var_smoothing': 0.01}\n",
      "Best Cross-Validation Accuracy: 0.8384\n"
     ]
    }
   ],
   "source": [
    "print(\"-- Tuning Naive Bayes --\")\n",
    "\n",
    "nb_model= GaussianNB()\n",
    "\n",
    "# Defining the hyperparameter\n",
    "# var_smoothing--> A stability paramter. Helps when a feature has zero variance.\n",
    "param_grid_nb={\n",
    "    'var_smoothing': np.logspace(0, -9, num=100)\n",
    "}\n",
    "\n",
    "# Setting up GridSeachCV\n",
    "grid_search_nb= GridSearchCV(estimator=nb_model, param_grid=param_grid_nb, cv=5, scoring='accuracy', n_jobs= -1, verbose=1)\n",
    "\n",
    "# Fitting the grid search to the data\n",
    "grid_search_nb.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best Parameters for Naive Bayes: {grid_search_nb.best_params_}\")\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search_nb.best_score_:.4f}\")\n",
    "\n",
    "# Save best model found by the search\n",
    "best_nb_model = grid_search_nb.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531895b5-1a42-4072-8372-74046f4f3f0b",
   "metadata": {},
   "source": [
    "## 7. Final Evaluation Of Tuned Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff50a26a-0aca-44f3-950d-844f8712f57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Evaluating Tuned Models on Test Set --\n",
      "\n",
      "*** Tuned Logistic Regression Report (Test Set): ***\n",
      "\n",
      "Accuracy: 0.8525\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.86      0.84        28\n",
      "           1       0.88      0.85      0.86        33\n",
      "\n",
      "    accuracy                           0.85        61\n",
      "   macro avg       0.85      0.85      0.85        61\n",
      "weighted avg       0.85      0.85      0.85        61\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "*** Tuned Naive Bayes Report (Test Set): ***\n",
      "\n",
      "Accuracy: 0.8525\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.86      0.84        28\n",
      "           1       0.88      0.85      0.86        33\n",
      "\n",
      "    accuracy                           0.85        61\n",
      "   macro avg       0.85      0.85      0.85        61\n",
      "weighted avg       0.85      0.85      0.85        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"-- Evaluating Tuned Models on Test Set --\")\n",
    "\n",
    "# Retrieving the best models found by GridSearchCV\n",
    "best_lr_model= grid_search_lr.best_estimator_\n",
    "best_nb_model= grid_search_nb.best_estimator_\n",
    "\n",
    "# Evaluating the tuned Logistic Regression model on the test set\n",
    "y_pred_lr_tuned= best_lr_model.predict(X_test)\n",
    "print(\"\\n*** Tuned Logistic Regression Report (Test Set): ***\")\n",
    "print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred_lr_tuned):.4f}\")\n",
    "print(classification_report(y_test, y_pred_lr_tuned))\n",
    "\n",
    "print(\"--\"*30)\n",
    "\n",
    "# Evaluating the tuned Naive Bayes model on the test set\n",
    "y_pred_nb_tuned= best_nb_model.predict(X_test)\n",
    "print(\"\\n*** Tuned Naive Bayes Report (Test Set): ***\")\n",
    "print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred_nb_tuned):.4f}\")\n",
    "print(classification_report(y_test, y_pred_nb_tuned))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77111a6a-43b7-4aff-9dfe-9a7557f35dc5",
   "metadata": {},
   "source": [
    "## 8. Save The Best Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4acc2c97-450d-4f7a-825d-7d4935af2496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model (Tuned Logistic Regression) has been saved to '../models/final_model.pkl'.\n"
     ]
    }
   ],
   "source": [
    "best_model = grid_search_lr.best_estimator_\n",
    "\n",
    "joblib.dump(best_model, '../models/final_model.pkl')\n",
    "print(\"Best model (Tuned Logistic Regression) has been saved to '../models/final_model.pkl'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a65500-f3b3-4510-a894-3f5c1f2915d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
