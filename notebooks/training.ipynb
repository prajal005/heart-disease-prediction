{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a44d7202-0ab7-4f39-9302-9e796776b708",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f2250cb-4f8a-4505-a7ad-3e364ac75d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e6f069-a708-411d-85bb-2b83e3efad09",
   "metadata": {},
   "source": [
    "### Importing Classic ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63573844-8c84-4364-ac0e-7df2c742efcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b5a08d-f4a7-4425-9d79-01abb2c4ab37",
   "metadata": {},
   "source": [
    "## 2. Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6cb9aa3-c3f2-48f4-adff-403c4757f400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data loaded successfully.\n",
      "   num__age  num__trestbps  num__chol  num__thalach  num__oldpeak  cat__sex_1  \\\n",
      "0  0.697674       0.245283   0.273973      0.201613      0.354839         1.0   \n",
      "1  0.534884       0.150943   0.171233      0.443548      0.241935         1.0   \n",
      "2  0.604651       0.339623   0.182648      0.491935      0.387097         1.0   \n",
      "3  0.534884       0.433962   0.150685      0.620968      0.064516         1.0   \n",
      "4  0.674419       0.132075   0.326484      0.790323      0.290323         0.0   \n",
      "\n",
      "   cat__cp_1  cat__cp_2  cat__cp_3  cat__fbs_1  ...  cat__exang_1  \\\n",
      "0        0.0        0.0        0.0         0.0  ...           1.0   \n",
      "1        0.0        0.0        0.0         0.0  ...           1.0   \n",
      "2        0.0        0.0        0.0         0.0  ...           1.0   \n",
      "3        0.0        0.0        0.0         0.0  ...           0.0   \n",
      "4        0.0        0.0        0.0         0.0  ...           1.0   \n",
      "\n",
      "   cat__slope_1  cat__slope_2  cat__ca_1  cat__ca_2  cat__ca_3  cat__thal_1  \\\n",
      "0           0.0           0.0        1.0        0.0        0.0          0.0   \n",
      "1           1.0           0.0        0.0        0.0        0.0          1.0   \n",
      "2           1.0           0.0        0.0        1.0        0.0          0.0   \n",
      "3           1.0           0.0        0.0        0.0        0.0          1.0   \n",
      "4           1.0           0.0        0.0        1.0        0.0          0.0   \n",
      "\n",
      "   cat__thal_2  cat__thal_3  target  \n",
      "0          1.0          0.0       0  \n",
      "1          0.0          0.0       1  \n",
      "2          0.0          1.0       0  \n",
      "3          0.0          0.0       1  \n",
      "4          1.0          0.0       0  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df = pd.read_csv('../data/processed/cleaned_test_data.csv')\n",
    "    print(\"Processed data loaded successfully.\")\n",
    "    print(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'cleaned_test_data.csv' not found in 'data/processed/'.\")\n",
    "    print(\"Please ensure you have run the model_training script first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95ba3b6-2d31-4fa2-af46-978d52737c3a",
   "metadata": {},
   "source": [
    "## 3. Seperating Features and Target & Spliting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26874c10-7b53-4547-8c6d-92b8fa3adb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (164, 21)\n",
      "Testing set shape: (41, 21)\n"
     ]
    }
   ],
   "source": [
    "X= df.drop('target', axis=1)\n",
    "y= df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720de0a6-99b0-4ab1-9af5-c92706069f13",
   "metadata": {},
   "source": [
    "## 4. Model Development & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bd59804-95f5-4327-bf6e-f94566ede88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression\n",
      "Accuracy: 0.68\n",
      "Precision: 0.70\n",
      "Recall: 0.67\n",
      "F1-Score: 0.68\n",
      "\n",
      "\n",
      "Model: K-Nearest Neighbors\n",
      "Accuracy: 0.73\n",
      "Precision: 0.73\n",
      "Recall: 0.76\n",
      "F1-Score: 0.74\n",
      "\n",
      "\n",
      "Model: Support Vector Machine\n",
      "Accuracy: 0.73\n",
      "Precision: 0.75\n",
      "Recall: 0.71\n",
      "F1-Score: 0.73\n",
      "\n",
      "\n",
      "Model: Naive Bayes\n",
      "Accuracy: 0.63\n",
      "Precision: 0.59\n",
      "Recall: 0.90\n",
      "F1-Score: 0.72\n",
      "\n",
      "\n",
      "Model: Decision Tree\n",
      "Accuracy: 0.80\n",
      "Precision: 0.84\n",
      "Recall: 0.76\n",
      "F1-Score: 0.80\n",
      "\n",
      "\n",
      "Model: Random Forest\n",
      "Accuracy: 0.73\n",
      "Precision: 0.73\n",
      "Recall: 0.76\n",
      "F1-Score: 0.74\n",
      "\n",
      "\n",
      "Model: Grandient Boosting\n",
      "Accuracy: 0.76\n",
      "Precision: 0.76\n",
      "Recall: 0.76\n",
      "F1-Score: 0.76\n",
      "\n",
      "\n",
      "Model: XGBoost\n",
      "Accuracy: 0.76\n",
      "Precision: 0.79\n",
      "Recall: 0.71\n",
      "F1-Score: 0.75\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models ={\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Support Vector Machine\": SVC(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Grandient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred= model.predict(X_test)\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "    print(f\"Precision: {precision_score(y_test, y_pred):.2f}\")\n",
    "    print(f\"Recall: {recall_score(y_test, y_pred):.2f}\")\n",
    "    print(f\"F1-Score: {f1_score(y_test, y_pred):.2f}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b24c05b-b661-4131-a8aa-91bab951e687",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9385364-92d8-4537-a77f-9fe1d94d417f",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96943e9b-89b1-48bd-91e7-3327ca61d2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Tuning Logistic Regression ===\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "\n",
      "Best Parameters for Logistic Regression:\n",
      "{'C': 10, 'penalty': 'l2'}\n",
      "Best Cross-Validation Accuracy: 0.8778\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Tuning Logistic Regression ===\")\n",
    "\n",
    "lr_model = LogisticRegression(random_state=42, solver='liblinear', max_iter=1000)\n",
    "\n",
    "param_grid_lr = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "grid_search_lr = GridSearchCV(estimator=lr_model, param_grid=param_grid_lr, \n",
    "                              cv=5, n_jobs=-1, verbose=1, scoring='accuracy')\n",
    "\n",
    "grid_search_lr.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest Parameters for Logistic Regression:\")\n",
    "print(grid_search_lr.best_params_)\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search_lr.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5418919-31e0-42e8-8874-e31c12f69979",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning for K-Nearest Neighbors(KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6df2cd44-a706-45ea-b92c-91a34af3ca2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Tuning K-Nearest Neighbors (KNN) ===\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "Best Parameters for K-Nearest Neighbors:\n",
      "{'metric': 'euclidean', 'n_neighbors': 11, 'weights': 'distance'}\n",
      "Best Cross-Validation Accuracy: 0.9019\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Tuning K-Nearest Neighbors (KNN) ===\")\n",
    "\n",
    "knn_model = KNeighborsClassifier()\n",
    "\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "}\n",
    "\n",
    "grid_search_knn = GridSearchCV(estimator=knn_model, param_grid=param_grid_knn, \n",
    "                               cv=5, n_jobs=-1, verbose=1, scoring='accuracy')\n",
    "\n",
    "grid_search_knn.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest Parameters for K-Nearest Neighbors:\")\n",
    "print(grid_search_knn.best_params_)\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search_knn.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0022e65-bd46-454b-92c5-ac20f9f7e260",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning for Support Vector Machine(SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f076509a-f9c9-4773-b2ba-6fba3cc146af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Tuning Support Vector Machine (SVM) ===\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "\n",
      "Best Parameters for Support Vector Machine:\n",
      "{'C': 1, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "Best Cross-Validation Accuracy: 0.8839\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Tuning Support Vector Machine (SVM) ===\")\n",
    "\n",
    "svm_model = SVC(random_state=42)\n",
    "\n",
    "param_grid_svm = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "grid_search_svm = GridSearchCV(estimator=svm_model, param_grid=param_grid_svm, \n",
    "                               cv=5, n_jobs=-1, verbose=1, scoring='accuracy')\n",
    "\n",
    "grid_search_svm.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest Parameters for Support Vector Machine:\")\n",
    "print(grid_search_svm.best_params_)\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search_svm.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4ce27e-8b10-444b-a39e-e124546848cf",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning for Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88763bb4-6735-4308-b2e7-96c62e123373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Tuning Naive Bayes ===\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "\n",
      "Best Parameters for Naive Bayes:\n",
      "{'var_smoothing': 0.02848035868435802}\n",
      "Best Cross-Validation Accuracy: 0.8477\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Tuning Naive Bayes ===\")\n",
    "\n",
    "nb_model = GaussianNB()\n",
    "\n",
    "param_grid_nb = {\n",
    "    'var_smoothing': np.logspace(0, -9, num=100)\n",
    "}\n",
    "\n",
    "grid_search_nb = GridSearchCV(estimator=nb_model, param_grid=param_grid_nb, \n",
    "                              cv=5, n_jobs=-1, verbose=1, scoring='accuracy')\n",
    "\n",
    "grid_search_nb.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest Parameters for Naive Bayes:\")\n",
    "print(grid_search_nb.best_params_)\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search_nb.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d4bd18-8474-4f81-a572-54e40294e2f8",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning for Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c120b729-d7ca-4271-9252-84849530ab9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tuning Decision Tree ===\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Best Parameters for Decision Tree:\n",
      "{'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Best Cross-Validation Accuracy: 0.8108\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Tuning Decision Tree ===\")\n",
    "\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "param_grid_dt = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [3, 5, 7, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search_dt = GridSearchCV(estimator=dt_model, param_grid=param_grid_dt, \n",
    "                              cv=5, n_jobs=-1, verbose=1, scoring='accuracy')\n",
    "\n",
    "grid_search_dt.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest Parameters for Decision Tree:\")\n",
    "print(grid_search_dt.best_params_)\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search_dt.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dd45c3-0a79-41d1-8b3e-77621a1467f9",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f900684e-2c38-4f15-91ca-594e199bc63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Tuning Random Forest ===\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "\n",
      "Best Parameters for Random Forest:\n",
      "{'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Best Cross-Validation Accuracy: 0.8902\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Tuning Random Forest ===\")\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search_rf = GridSearchCV(estimator=rf_model, param_grid=param_grid_rf, \n",
    "                              cv=5, n_jobs=-1, verbose=1, scoring='accuracy')\n",
    "\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest Parameters for Random Forest:\")\n",
    "print(grid_search_rf.best_params_)\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search_rf.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f075f3d4-2287-40c7-ba9b-6c21a9bb1bf2",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning for Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "376a770d-b7d6-4fdf-9e73-3da19a6b3ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Tuning Gradient Boosting ===\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "\n",
      "Best Parameters for Gradient Boosting:\n",
      "{'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 50}\n",
      "Best Cross-Validation Accuracy: 0.9025\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Tuning Gradient Boosting ===\")\n",
    "\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "grid_search_gb = GridSearchCV(estimator=gb_model, param_grid=param_grid_gb, \n",
    "                              cv=5, n_jobs=-1, verbose=1, scoring='accuracy')\n",
    "\n",
    "grid_search_gb.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest Parameters for Gradient Boosting:\")\n",
    "print(grid_search_gb.best_params_)\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search_gb.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3efa71-7842-42db-9e24-07597acf6314",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5e23eed-b00d-4e2f-9ab6-71f84fd8201d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Tuning XGBoost ===\n",
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
      "\n",
      "Best Parameters for XGBoost:\n",
      "{'colsample_bytree': 0.7, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Best Cross-Validation Accuracy: 0.9085\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Tuning XGBoost ===\")\n",
    "\n",
    "xgb_model = XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.7, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "grid_search_xgb = GridSearchCV(estimator=xgb_model, param_grid=param_grid_xgb, \n",
    "                               cv=5, n_jobs=-1, verbose=1, scoring='accuracy')\n",
    "\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest Parameters for XGBoost:\")\n",
    "print(grid_search_xgb.best_params_)\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search_xgb.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531895b5-1a42-4072-8372-74046f4f3f0b",
   "metadata": {},
   "source": [
    "## 6. Final Evaluation Of Tuned Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff50a26a-0aca-44f3-950d-844f8712f57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Evaluating Tuned Models on Test Set ===\n",
      "\n",
      "*** Logistic Regression Report (Test Set): ***\n",
      "Accuracy: 0.7561\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.80      0.76        20\n",
      "           1       0.79      0.71      0.75        21\n",
      "\n",
      "    accuracy                           0.76        41\n",
      "   macro avg       0.76      0.76      0.76        41\n",
      "weighted avg       0.76      0.76      0.76        41\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "*** KNN Report (Test Set): ***\n",
      "Accuracy: 0.7805\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.85      0.79        20\n",
      "           1       0.83      0.71      0.77        21\n",
      "\n",
      "    accuracy                           0.78        41\n",
      "   macro avg       0.79      0.78      0.78        41\n",
      "weighted avg       0.79      0.78      0.78        41\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "*** SVM Report (Test Set): ***\n",
      "Accuracy: 0.6829\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.65      0.67        20\n",
      "           1       0.68      0.71      0.70        21\n",
      "\n",
      "    accuracy                           0.68        41\n",
      "   macro avg       0.68      0.68      0.68        41\n",
      "weighted avg       0.68      0.68      0.68        41\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "*** Naive Bayes Report (Test Set): ***\n",
      "Accuracy: 0.7805\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.85      0.79        20\n",
      "           1       0.83      0.71      0.77        21\n",
      "\n",
      "    accuracy                           0.78        41\n",
      "   macro avg       0.79      0.78      0.78        41\n",
      "weighted avg       0.79      0.78      0.78        41\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "*** Decision Tree Report (Test Set): ***\n",
      "Accuracy: 0.8537\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.95      0.86        20\n",
      "           1       0.94      0.76      0.84        21\n",
      "\n",
      "    accuracy                           0.85        41\n",
      "   macro avg       0.87      0.86      0.85        41\n",
      "weighted avg       0.87      0.85      0.85        41\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "*** Random Forest Report (Test Set): ***\n",
      "Accuracy: 0.7561\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.75      0.75        20\n",
      "           1       0.76      0.76      0.76        21\n",
      "\n",
      "    accuracy                           0.76        41\n",
      "   macro avg       0.76      0.76      0.76        41\n",
      "weighted avg       0.76      0.76      0.76        41\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "*** Gradient Boosting Report (Test Set): ***\n",
      "Accuracy: 0.7561\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.75      0.75        20\n",
      "           1       0.76      0.76      0.76        21\n",
      "\n",
      "    accuracy                           0.76        41\n",
      "   macro avg       0.76      0.76      0.76        41\n",
      "weighted avg       0.76      0.76      0.76        41\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "*** XGBoost Report (Test Set): ***\n",
      "Accuracy: 0.7317\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.75      0.73        20\n",
      "           1       0.75      0.71      0.73        21\n",
      "\n",
      "    accuracy                           0.73        41\n",
      "   macro avg       0.73      0.73      0.73        41\n",
      "weighted avg       0.73      0.73      0.73        41\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Evaluating Tuned Models on Test Set ===\")\n",
    "\n",
    "# Retrieving the best models found by GridSearchCV\n",
    "best_lr_model = grid_search_lr.best_estimator_\n",
    "best_knn_model = grid_search_knn.best_estimator_\n",
    "best_svm_model = grid_search_svm.best_estimator_\n",
    "best_nb_model = grid_search_nb.best_estimator_\n",
    "best_dt_model = grid_search_dt.best_estimator_\n",
    "best_rf_model = grid_search_rf.best_estimator_\n",
    "best_gb_model = grid_search_gb.best_estimator_\n",
    "best_xgb_model = grid_search_xgb.best_estimator_\n",
    "\n",
    "# Creating a list of models for evaluation\n",
    "models = {\n",
    "    \"Logistic Regression\": best_lr_model,\n",
    "    \"KNN\": best_knn_model,\n",
    "    \"SVM\": best_svm_model,\n",
    "    \"Naive Bayes\": best_nb_model,\n",
    "    \"Decision Tree\": best_dt_model,\n",
    "    \"Random Forest\": best_rf_model,\n",
    "    \"Gradient Boosting\": best_gb_model,\n",
    "    \"XGBoost\": best_xgb_model\n",
    "}\n",
    "\n",
    "# Evaluating each model\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"\\n*** {name} Report (Test Set): ***\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"--\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77111a6a-43b7-4aff-9dfe-9a7557f35dc5",
   "metadata": {},
   "source": [
    "## 8. Save All Hyper Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4acc2c97-450d-4f7a-825d-7d4935af2496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved logistic_regression to models/tuned/logistic_regression.pkl\n",
      "Saved knn to models/tuned/knn.pkl\n",
      "Saved svm to models/tuned/svm.pkl\n",
      "Saved naive_bayes to models/tuned/naive_bayes.pkl\n",
      "Saved decision_tree to models/tuned/decision_tree.pkl\n",
      "Saved random_forest to models/tuned/random_forest.pkl\n",
      "Saved gradient_boosting to models/tuned/gradient_boosting.pkl\n",
      "Saved xgboost to models/tuned/xgboost.pkl\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "models = {\n",
    "    \"logistic_regression\": best_lr_model,\n",
    "    \"knn\": best_knn_model,\n",
    "    \"svm\": best_svm_model,\n",
    "    \"naive_bayes\": best_nb_model,\n",
    "    \"decision_tree\": best_dt_model,\n",
    "    \"random_forest\": best_rf_model,\n",
    "    \"gradient_boosting\": best_gb_model,\n",
    "    \"xgboost\": best_xgb_model\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    joblib.dump(model, f\"../models/tuned/{name}.pkl\")\n",
    "    print(f\"Saved {name} to models/tuned/{name}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a65500-f3b3-4510-a894-3f5c1f2915d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
