{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a44d7202-0ab7-4f39-9302-9e796776b708",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f2250cb-4f8a-4505-a7ad-3e364ac75d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e6f069-a708-411d-85bb-2b83e3efad09",
   "metadata": {},
   "source": [
    "### Importing Classic ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63573844-8c84-4364-ac0e-7df2c742efcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2ae66f-1596-40b2-9fe3-f8f4f1b76f26",
   "metadata": {},
   "source": [
    "### Import Suppoting file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18c6013f-591f-456e-9764-5b8597186ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir= os.path.abspath('')\n",
    "\n",
    "project_root= os.path.dirname(current_dir)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from scripts.preprocessing import create_preprocessor\n",
    "from scripts.model_utils import load_data, save_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b5a08d-f4a7-4425-9d79-01abb2c4ab37",
   "metadata": {},
   "source": [
    "## 2. Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6cb9aa3-c3f2-48f4-adff-403c4757f400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
      "0   52    1   0       125   212    0        1      168      0      1.0      2   \n",
      "1   53    1   0       140   203    1        0      155      1      3.1      0   \n",
      "2   70    1   0       145   174    0        1      125      1      2.6      0   \n",
      "3   61    1   0       148   203    0        1      161      0      0.0      2   \n",
      "4   62    0   0       138   294    1        1      106      0      1.9      1   \n",
      "\n",
      "   ca  thal  target  \n",
      "0   2     3       0  \n",
      "1   0     3       0  \n",
      "2   0     3       0  \n",
      "3   1     3       0  \n",
      "4   3     2       0  \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df = pd.read_csv('../data/raw/heart.csv')\n",
    "    print(\"Data loaded successfully.\")\n",
    "    print(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'heart.csv' not found in 'data/raw/'.\")\n",
    "    print(\"Please ensure the raw data is in the '../data/raw' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95ba3b6-2d31-4fa2-af46-978d52737c3a",
   "metadata": {},
   "source": [
    "## 3. Spliting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d182ee47-212e-4610-84b8-e9b8e620420e",
   "metadata": {},
   "source": [
    "### Spliting Data into training+validation and final test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26874c10-7b53-4547-8c6d-92b8fa3adb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data splited into training+validation and final test set\n"
     ]
    }
   ],
   "source": [
    "X= df.drop('target', axis=1)\n",
    "y= df['target']\n",
    "\n",
    "X_train_val, X_final_test, y_train_val, y_final_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(\"Data splited into training+validation and final test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac75ed9d-f360-4c42-b27d-315d751bd51a",
   "metadata": {},
   "source": [
    "### Spliting training+validation data into seperate sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9f5d72d-7ca4-400d-a29f-866a8950a456",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val= train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42, stratify=y_train_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf90a5a-01fe-4205-93ba-84ab5e430ca8",
   "metadata": {},
   "source": [
    "### Save the unseen test data to be used in the final evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15250ea0-4eba-4d46-a327-f4dda8cbaecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final unseen test data saved to ../data/processed/final_test_data.csv\n",
      "Training set shape: (615, 13)\n",
      "Validation set shape: (205, 13)\n",
      "Final Test set shape: (205, 13)\n"
     ]
    }
   ],
   "source": [
    "final_test_df = X_final_test.copy()\n",
    "final_test_df['target'] = y_final_test\n",
    "final_test_output_path = '../data/processed/final_test_data.csv'\n",
    "os.makedirs(os.path.dirname(final_test_output_path), exist_ok=True)\n",
    "final_test_df.to_csv(final_test_output_path, index=False)\n",
    "print(f\"Final unseen test data saved to {final_test_output_path}\")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n",
    "print(f\"Final Test set shape: {X_final_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720de0a6-99b0-4ab1-9af5-c92706069f13",
   "metadata": {},
   "source": [
    "## 4. Model Development & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592c7832-5dec-42b1-9cd5-aa7325e74c27",
   "metadata": {},
   "source": [
    "### Evaluation of Base Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bd59804-95f5-4327-bf6e-f94566ede88d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression\n",
      "Accuracy: 0.84\n",
      "Precision: 0.86\n",
      "Recall: 0.83\n",
      "F1-Score: 0.84\n",
      "\n",
      "\n",
      "Model: K-Nearest Neighbors\n",
      "Accuracy: 0.81\n",
      "Precision: 0.81\n",
      "Recall: 0.84\n",
      "F1-Score: 0.82\n",
      "\n",
      "\n",
      "Model: Support Vector Machine\n",
      "Accuracy: 0.89\n",
      "Precision: 0.90\n",
      "Recall: 0.89\n",
      "F1-Score: 0.89\n",
      "\n",
      "\n",
      "Model: Naive Bayes\n",
      "Accuracy: 0.84\n",
      "Precision: 0.85\n",
      "Recall: 0.85\n",
      "F1-Score: 0.85\n",
      "\n",
      "\n",
      "Model: Decision Tree\n",
      "Accuracy: 0.95\n",
      "Precision: 0.95\n",
      "Recall: 0.94\n",
      "F1-Score: 0.95\n",
      "\n",
      "\n",
      "Model: Random Forest\n",
      "Accuracy: 0.98\n",
      "Precision: 1.00\n",
      "Recall: 0.96\n",
      "F1-Score: 0.98\n",
      "\n",
      "\n",
      "Model: Grandient Boosting\n",
      "Accuracy: 0.94\n",
      "Precision: 0.94\n",
      "Recall: 0.93\n",
      "F1-Score: 0.94\n",
      "\n",
      "\n",
      "Model: XGBoost\n",
      "Accuracy: 0.95\n",
      "Precision: 0.94\n",
      "Recall: 0.96\n",
      "F1-Score: 0.95\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preprocessor= create_preprocessor()\n",
    "\n",
    "models ={\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Support Vector Machine\": SVC(random_state=42),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Grandient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    pipeline= Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred= pipeline.predict(X_val) # Predicts on validation set\n",
    "    \n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_val, y_pred):.2f}\")\n",
    "    print(f\"Precision: {precision_score(y_val, y_pred):.2f}\")\n",
    "    print(f\"Recall: {recall_score(y_val, y_pred):.2f}\")\n",
    "    print(f\"F1-Score: {f1_score(y_val, y_pred):.2f}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f03efc5-1913-42e8-87a7-1bf894ce7b34",
   "metadata": {},
   "source": [
    "### Analysis of Base Models\n",
    "* Based on the baseline evaluation, the Random Forest model with its default parameters shows the strongest performance, achieving a high Accuracy of 0.98, a perfect Precision of 1.00, a strong Recall of 0.96, and a top-tier F1-Score of 0.98.\n",
    "* The Decision Tree and XGBoost models also perform well, with high scores across all metrics. \n",
    "* This indicates that ensemble methods and tree-based models are well-suited for this dataset, even before any hyperparameter tuning is performed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b24c05b-b661-4131-a8aa-91bab951e687",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9385364-92d8-4537-a77f-9fe1d94d417f",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96943e9b-89b1-48bd-91e7-3327ca61d2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Tuning Logistic Regression ===\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "\n",
      "Best Parameters for Logistic Regression:\n",
      "{'classifier__C': 10, 'classifier__penalty': 'l1', 'classifier__solver': 'liblinear'}\n",
      "Best Cross-Validation Accuracy: 0.8683\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Tuning Logistic Regression ===\")\n",
    "\n",
    "preprocessor= create_preprocessor()\n",
    "pipeline_lr= Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "param_grid_lr = {\n",
    "    'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'classifier__penalty': ['l1', 'l2'],\n",
    "    'classifier__solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "grid_search_lr = GridSearchCV(estimator=pipeline_lr, param_grid=param_grid_lr, cv=5, n_jobs=-1, verbose=1, scoring='accuracy')\n",
    "grid_search_lr.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest Parameters for Logistic Regression:\")\n",
    "print(grid_search_lr.best_params_)\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search_lr.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5418919-31e0-42e8-8874-e31c12f69979",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning for K-Nearest Neighbors(KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6df2cd44-a706-45ea-b92c-91a34af3ca2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Tuning K-Nearest Neighbors (KNN) ===\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "Best Parameters for K-Nearest Neighbors:\n",
      "{'classifier__metric': 'euclidean', 'classifier__n_neighbors': 11, 'classifier__weights': 'distance'}\n",
      "Best Cross-Validation Accuracy: 0.9561\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Tuning K-Nearest Neighbors (KNN) ===\")\n",
    "\n",
    "preprocessor= create_preprocessor()\n",
    "pipeline_knn= Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "param_grid_knn = {\n",
    "    'classifier__n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'classifier__weights': ['uniform', 'distance'],\n",
    "    'classifier__metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "}\n",
    "\n",
    "grid_search_knn = GridSearchCV(estimator=pipeline_knn, param_grid=param_grid_knn, cv=5, n_jobs=-1, verbose=1, scoring='accuracy')\n",
    "grid_search_knn.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest Parameters for K-Nearest Neighbors:\")\n",
    "print(grid_search_knn.best_params_)\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search_knn.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0022e65-bd46-454b-92c5-ac20f9f7e260",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning for Support Vector Machine(SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f076509a-f9c9-4773-b2ba-6fba3cc146af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Tuning Support Vector Machine (SVM) ===\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "\n",
      "Best Parameters for Support Vector Machine:\n",
      "{'classifier__C': 100, 'classifier__gamma': 'scale', 'classifier__kernel': 'poly'}\n",
      "Best Cross-Validation Accuracy: 0.9480\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Tuning Support Vector Machine (SVM) ===\")\n",
    "\n",
    "preprocessor= create_preprocessor()\n",
    "pipeline_svm= Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SVC(random_state=42, probability=True))\n",
    "])\n",
    "                       \n",
    "param_grid_svm = {\n",
    "    'classifier__C': [0.1, 1, 10, 100],\n",
    "    'classifier__kernel': ['linear', 'rbf', 'poly'],\n",
    "    'classifier__gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "grid_search_svm = GridSearchCV(estimator=pipeline_svm, param_grid=param_grid_svm, cv=5, n_jobs=-1, verbose=1, scoring='accuracy')\n",
    "grid_search_svm.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest Parameters for Support Vector Machine:\")\n",
    "print(grid_search_svm.best_params_)\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search_svm.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4ce27e-8b10-444b-a39e-e124546848cf",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning for Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88763bb4-6735-4308-b2e7-96c62e123373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Tuning Naive Bayes ===\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "\n",
      "Best Parameters for Naive Bayes:\n",
      "{'classifier__var_smoothing': 0.1873817422860384}\n",
      "Best Cross-Validation Accuracy: 0.8439\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Tuning Naive Bayes ===\")\n",
    "\n",
    "preprocessor= create_preprocessor()\n",
    "pipeline_nb= Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GaussianNB())\n",
    "])\n",
    "\n",
    "param_grid_nb = {\n",
    "    'classifier__var_smoothing': np.logspace(0, -9, num=100)\n",
    "}\n",
    "\n",
    "grid_search_nb = GridSearchCV(estimator=pipeline_nb, param_grid=param_grid_nb, cv=5, n_jobs=-1, verbose=1, scoring='accuracy')\n",
    "grid_search_nb.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest Parameters for Naive Bayes:\")\n",
    "print(grid_search_nb.best_params_)\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search_nb.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d4bd18-8474-4f81-a572-54e40294e2f8",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning for Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c120b729-d7ca-4271-9252-84849530ab9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tuning Decision Tree ===\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "\n",
      "Best Parameters for Decision Tree:\n",
      "{'classifier__criterion': 'gini', 'classifier__max_depth': None, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2}\n",
      "Best Cross-Validation Accuracy: 0.9398\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Tuning Decision Tree ===\")\n",
    "\n",
    "preprocessor= create_preprocessor()\n",
    "pipeline_dt= Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "param_grid_dt = {\n",
    "    'classifier__criterion': ['gini', 'entropy'],\n",
    "    'classifier__max_depth': [3, 5, 7, 10, None],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search_dt = GridSearchCV(estimator=pipeline_dt, param_grid=param_grid_dt, cv=5, n_jobs=-1, verbose=1, scoring='accuracy')\n",
    "grid_search_dt.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest Parameters for Decision Tree:\")\n",
    "print(grid_search_dt.best_params_)\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search_dt.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dd45c3-0a79-41d1-8b3e-77621a1467f9",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f900684e-2c38-4f15-91ca-594e199bc63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Tuning Random Forest ===\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "\n",
      "Best Parameters for Random Forest:\n",
      "{'classifier__max_depth': None, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 100}\n",
      "Best Cross-Validation Accuracy: 0.9528\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Tuning Random Forest ===\")\n",
    "\n",
    "preprocessor= create_preprocessor()\n",
    "pipeline_rf= Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "param_grid_rf = {\n",
    "    'classifier__n_estimators': [50, 100, 200],\n",
    "    'classifier__max_depth': [5, 10, None],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search_rf = GridSearchCV(estimator=pipeline_rf, param_grid=param_grid_rf, cv=5, n_jobs=-1, verbose=1, scoring='accuracy')\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest Parameters for Random Forest:\")\n",
    "print(grid_search_rf.best_params_)\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search_rf.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f075f3d4-2287-40c7-ba9b-6c21a9bb1bf2",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning for Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "376a770d-b7d6-4fdf-9e73-3da19a6b3ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Tuning Gradient Boosting ===\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "\n",
      "Best Parameters for Gradient Boosting:\n",
      "{'classifier__learning_rate': 0.2, 'classifier__max_depth': 5, 'classifier__n_estimators': 50}\n",
      "Best Cross-Validation Accuracy: 0.9593\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Tuning Gradient Boosting ===\")\n",
    "\n",
    "preprocessor= create_preprocessor()\n",
    "pipeline_gb= Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GradientBoostingClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "param_grid_gb = {\n",
    "    'classifier__n_estimators': [50, 100, 200],\n",
    "    'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'classifier__max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "grid_search_gb = GridSearchCV(estimator=pipeline_gb, param_grid=param_grid_gb, cv=5, n_jobs=-1, verbose=1, scoring='accuracy')\n",
    "grid_search_gb.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest Parameters for Gradient Boosting:\")\n",
    "print(grid_search_gb.best_params_)\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search_gb.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3efa71-7842-42db-9e24-07597acf6314",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d5e23eed-b00d-4e2f-9ab6-71f84fd8201d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Tuning XGBoost ===\n",
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
      "\n",
      "Best Parameters for XGBoost:\n",
      "{'classifier__colsample_bytree': 0.7, 'classifier__learning_rate': 0.2, 'classifier__max_depth': 3, 'classifier__n_estimators': 200, 'classifier__subsample': 0.7}\n",
      "Best Cross-Validation Accuracy: 0.9577\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Tuning XGBoost ===\")\n",
    "\n",
    "preprocessor= create_preprocessor()\n",
    "pipeline_xgb= Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', XGBClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'classifier__n_estimators': [50, 100, 200],\n",
    "    'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'classifier__max_depth': [3, 5, 7],\n",
    "    'classifier__subsample': [0.7, 0.8, 1.0],\n",
    "    'classifier__colsample_bytree': [0.7, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "grid_search_xgb = GridSearchCV(estimator=pipeline_xgb, param_grid=param_grid_xgb, cv=5, n_jobs=-1, verbose=1, scoring='accuracy')\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest Parameters for XGBoost:\")\n",
    "print(grid_search_xgb.best_params_)\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search_xgb.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531895b5-1a42-4072-8372-74046f4f3f0b",
   "metadata": {},
   "source": [
    "## 6. Final Evaluation Of Tuned Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff50a26a-0aca-44f3-950d-844f8712f57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Evaluating Tuned Models on Test Set ===\n",
      "\n",
      "*** Logistic Regression Report (Validation Set): ***\n",
      "Accuracy: 0.8585\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.88      0.86       100\n",
      "           1       0.88      0.84      0.86       105\n",
      "\n",
      "    accuracy                           0.86       205\n",
      "   macro avg       0.86      0.86      0.86       205\n",
      "weighted avg       0.86      0.86      0.86       205\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "*** KNN Report (Validation Set): ***\n",
      "Accuracy: 0.9805\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       100\n",
      "           1       1.00      0.96      0.98       105\n",
      "\n",
      "    accuracy                           0.98       205\n",
      "   macro avg       0.98      0.98      0.98       205\n",
      "weighted avg       0.98      0.98      0.98       205\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "*** SVM Report (Validation Set): ***\n",
      "Accuracy: 0.9707\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97       100\n",
      "           1       1.00      0.94      0.97       105\n",
      "\n",
      "    accuracy                           0.97       205\n",
      "   macro avg       0.97      0.97      0.97       205\n",
      "weighted avg       0.97      0.97      0.97       205\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "*** Naive Bayes Report (Validation Set): ***\n",
      "Accuracy: 0.8439\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.87      0.84       100\n",
      "           1       0.87      0.82      0.84       105\n",
      "\n",
      "    accuracy                           0.84       205\n",
      "   macro avg       0.84      0.84      0.84       205\n",
      "weighted avg       0.85      0.84      0.84       205\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "*** Decision Tree Report (Validation Set): ***\n",
      "Accuracy: 0.9463\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.95       100\n",
      "           1       0.95      0.94      0.95       105\n",
      "\n",
      "    accuracy                           0.95       205\n",
      "   macro avg       0.95      0.95      0.95       205\n",
      "weighted avg       0.95      0.95      0.95       205\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "*** Random Forest Report (Validation Set): ***\n",
      "Accuracy: 0.9805\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       100\n",
      "           1       1.00      0.96      0.98       105\n",
      "\n",
      "    accuracy                           0.98       205\n",
      "   macro avg       0.98      0.98      0.98       205\n",
      "weighted avg       0.98      0.98      0.98       205\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "*** Gradient Boosting Report (Validation Set): ***\n",
      "Accuracy: 0.9756\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98       100\n",
      "           1       0.99      0.96      0.98       105\n",
      "\n",
      "    accuracy                           0.98       205\n",
      "   macro avg       0.98      0.98      0.98       205\n",
      "weighted avg       0.98      0.98      0.98       205\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "*** XGBoost Report (Validation Set): ***\n",
      "Accuracy: 0.9659\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.97       100\n",
      "           1       0.99      0.94      0.97       105\n",
      "\n",
      "    accuracy                           0.97       205\n",
      "   macro avg       0.97      0.97      0.97       205\n",
      "weighted avg       0.97      0.97      0.97       205\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Evaluating Tuned Models on Test Set ===\")\n",
    "\n",
    "# Retrieving the best models found by GridSearchCV\n",
    "best_lr_model = grid_search_lr.best_estimator_\n",
    "best_knn_model = grid_search_knn.best_estimator_\n",
    "best_svm_model = grid_search_svm.best_estimator_\n",
    "best_nb_model = grid_search_nb.best_estimator_\n",
    "best_dt_model = grid_search_dt.best_estimator_\n",
    "best_rf_model = grid_search_rf.best_estimator_\n",
    "best_gb_model = grid_search_gb.best_estimator_\n",
    "best_xgb_model = grid_search_xgb.best_estimator_\n",
    "\n",
    "# Creating a list of models for evaluation\n",
    "models = {\n",
    "    \"Logistic Regression\": best_lr_model,\n",
    "    \"KNN\": best_knn_model,\n",
    "    \"SVM\": best_svm_model,\n",
    "    \"Naive Bayes\": best_nb_model,\n",
    "    \"Decision Tree\": best_dt_model,\n",
    "    \"Random Forest\": best_rf_model,\n",
    "    \"Gradient Boosting\": best_gb_model,\n",
    "    \"XGBoost\": best_xgb_model\n",
    "}\n",
    "\n",
    "# Evaluating each model\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_val)\n",
    "    print(f\"\\n*** {name} Report (Validation Set): ***\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_val, y_pred):.4f}\")\n",
    "    print(classification_report(y_val, y_pred))\n",
    "    print(\"--\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d052a5-63d8-4511-b59f-850d5fbaffb0",
   "metadata": {},
   "source": [
    "### Analysis of Tuned Models\n",
    "* Top Performers: KNN and Random Forest are tied for the highest accuracy at 0.9805. Both models also have an F1-score of 0.98 and a perfect precision of 1.00 for the positive class.\n",
    "\n",
    "* Strong Contenders: Gradient Boosting and SVM follow closely behind, with high accuracy and F1-scores as well.\n",
    "\n",
    "* Average Performers: Logistic Regression and Naive Bayes show lower performance compared to the other models, suggesting they may not be the best fit for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77111a6a-43b7-4aff-9dfe-9a7557f35dc5",
   "metadata": {},
   "source": [
    "## 8. Save All Hyper Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4acc2c97-450d-4f7a-825d-7d4935af2496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved logistic_regression to models/tuned/logistic_regression.pkl\n",
      "Saved knn to models/tuned/knn.pkl\n",
      "Saved svm to models/tuned/svm.pkl\n",
      "Saved naive_bayes to models/tuned/naive_bayes.pkl\n",
      "Saved decision_tree to models/tuned/decision_tree.pkl\n",
      "Saved random_forest to models/tuned/random_forest.pkl\n",
      "Saved gradient_boosting to models/tuned/gradient_boosting.pkl\n",
      "Saved xgboost to models/tuned/xgboost.pkl\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "models_to_save = {\n",
    "    \"logistic_regression\": best_lr_model,\n",
    "    \"knn\": best_knn_model,\n",
    "    \"svm\": best_svm_model,\n",
    "    \"naive_bayes\": best_nb_model,\n",
    "    \"decision_tree\": best_dt_model,\n",
    "    \"random_forest\": best_rf_model,\n",
    "    \"gradient_boosting\": best_gb_model,\n",
    "    \"xgboost\": best_xgb_model\n",
    "}\n",
    "\n",
    "for name, model in models_to_save.items():\n",
    "    joblib.dump(model, f\"../models/tuned/{name}.pkl\")\n",
    "    print(f\"Saved {name} to models/tuned/{name}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a65500-f3b3-4510-a894-3f5c1f2915d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
